{"categories":[{"title":"Ansible","uri":"https://midiroot.com/categories/ansible/"},{"title":"Bash","uri":"https://midiroot.com/categories/bash/"},{"title":"Best Practices","uri":"https://midiroot.com/categories/best-practices/"},{"title":"Cryptsetup","uri":"https://midiroot.com/categories/cryptsetup/"},{"title":"HAProxy","uri":"https://midiroot.com/categories/haproxy/"},{"title":"Install","uri":"https://midiroot.com/categories/install/"},{"title":"journalctl","uri":"https://midiroot.com/categories/journalctl/"},{"title":"journald","uri":"https://midiroot.com/categories/journald/"},{"title":"Linux","uri":"https://midiroot.com/categories/linux/"},{"title":"Logrotate","uri":"https://midiroot.com/categories/logrotate/"},{"title":"LUKS","uri":"https://midiroot.com/categories/luks/"},{"title":"OSSEC","uri":"https://midiroot.com/categories/ossec/"},{"title":"SELinux","uri":"https://midiroot.com/categories/selinux/"},{"title":"Setup","uri":"https://midiroot.com/categories/setup/"},{"title":"Shell","uri":"https://midiroot.com/categories/shell/"},{"title":"Snippets","uri":"https://midiroot.com/categories/snippets/"}],"posts":[{"content":"Introduction Couple weeks ago, I took a journey to centralize all our infrastructure logs in one location for the seek of ease to teams who needs access such logs.\nDiversity is a good thing, but it's a challenge when you have multiple devices in your infrastructure that generate logs in different formats and standards. While looking at multiple solutions and comparing them.\nWe needed something that support Linux, Windows and dozens of network devices, a solution that easy to configure, automate and cost effective. With thousands of endpoints it must be of high performance and possible to scale when it is needed to. The quest lead us to the old rsyslog, it was the only solution that checked all the boxes with capability to deliver one million messages per second to local destinations and it is completely free.\nIn this post, I will cover some of the basic configurations to get you up an running with rsyslog and to make your configuration easier to automate and to change your rsyslog server as you add more endpoints.\nWorking with Rsyslog Installation rsyslog comes pre-installed in all major Linux distributions, if you need steps to install rsyslog, check Newbie guide to rsyslog to install from source, otherwise use your distro package manager to install it.\nDebian: sudo apt install rsyslog  RHEL: sudo yum install rsyslog  Configurations Rsyslogd's main configuration is typically found in /etc/rsyslog.conf, more configuration files can be placed in /etc/rsyslog.d/ for a modular configuration.\nFor this post, we will be creating modular configuration files for inputs, templates and rulesets.\nInputs In a rsyslog server, input is used to describe the message sources, input takes multiple parameters, we will be using few of them to define the port and type.\nI new config file called 01-inputs.conf create your first input:\ninput(type=\u0026quot;imudp\u0026quot; port=\u0026quot;514\u0026quot; ruleset=\u0026quot;remote_all\u0026quot;) input(type=\u0026quot;imtcp\u0026quot; port=\u0026quot;514\u0026quot; ruleset=\u0026quot;remote_all\u0026quot;)  With this; we created two inputs using default rsyslog port (514), one to accept TCP connections with type imtcp and the secoand one for UDP using imudp. For TCP, you can use imptcp which tailored for high performance and it is available since versions 4.7.3+, 5.5.8+.\nruleset is used to define the ruleset that will be used with this input, we will define one in our output config file.\nYou can add other inputs as you need as long as the port you picked is not used by another process.\nTemplates Templates allow you to specify the format of the message or used for dynamic file name generation which we will ne using in our 01-templates.conf file:\ntemplate(name=\u0026quot;catch_perhost\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/%FROMHOST%.log\u0026quot;)  We are using here a template of type string. list, subtree and plugin are also available as template types. We are generating dynamic file names using %FROMHOST% as a string the file name. you can change that and use static name if you want to send logs from a specific group of devices:\ntemplate(name=\u0026quot;catch_asa\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/asa.log\u0026quot;) template(name=\u0026quot;catch_vcenter\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/vcenter.log\u0026quot;) template(name=\u0026quot;catch_all\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/catch_all.log\u0026quot;)  Notice the template name change which will give us later more flexibility when it comes to our rulesets. When choosing template name please avoiding using RSYSLOG_ as they reserved for rsyslog use and will cause conflict.\nI am using catch_all template to process all unfiltered messages, this help in logging all messages that escape filters so I can review them and see what needs to be adjusted to filter them.\nRulesets Rulesets consists of a filter and action(s) to be executed when the filter evaluate to true. A filter evaluated using could a priority and/or facility, other property-based filters could be used like msg, syslogtag, hostname or host IP.\nIn our 03-rulesets.conf, we will define a ruleset for processing messages coming from an IP address that belongs to a vCenter host and places messages in vcenter.log file that we defined in our templates files.\nruleset(name=\u0026quot;remote_all\u0026quot; queue.type=\u0026quot;LinkedList\u0026quot; queue.size=\u0026quot;100000\u0026quot;) { if ($fromhost-ip == '192.168.33.11') then { action(type=\u0026quot;omfile\u0026quot; DynaFile=\u0026quot;catch_vcenter\u0026quot; FileCreateMode=\u0026quot;0664\u0026quot;) stop } # This setup the catch all logging action(type=\u0026quot;omfile\u0026quot; DynaFile=\u0026quot;catch_all\u0026quot;) }  A lot to unpack here, we are using remote_all as the ruleset name, that's the same values we used when defining what ruleset the input will be using. LinkedList is in-memory queue type. when using LinkedList queue; the memory is allocated only when needed which make this type of queue handle message bursts very well. In our filter, we check if the $fromhost-ip is equal to IP address of our vCenter, if true; we use omfile type to write messages to files using catch_vcenter as a dynamic file template. FileCreateMode sets permissions for the dynamic file we just created.\nConclusion Rsyslog is a powerful logging solution and we barely scratched the surface of all the hundreds of available options/parameters we could use for configuration. It's always a good practice to test configuration in your testing environment and see what rsyslog documentation has to offer. If you see that I could have something differently in my config, please reach out and let me know, I am always looking for improvement.\n","id":0,"section":"posts","summary":"Introduction Couple weeks ago, I took a journey to centralize all our infrastructure logs in one location for the seek of ease to teams who needs access such logs.\nDiversity is a good thing, but it's a challenge when you have multiple devices in your infrastructure that generate logs in different formats and standards. While looking at multiple solutions and comparing them.\nWe needed something that support Linux, Windows and dozens of network devices, a solution that easy to configure, automate and cost effective.","tags":["rsyslog Linux Guide"],"title":"Working With Rsyslog","uri":"https://midiroot.com/working-with-rsyslog/","year":"2019"},{"content":"intro In this post, I will demonstrate how we can install HAProxy from source code. According to Wikipedia, HAProxy was written and still maintained by Willy Tarreau since 2000. HAProxy is free open source software (FOSS), that provides a high availability load balancer and proxy server for TCP (Transmission Control Protocol) and HTTP (Hypertext Transfer Protocol) based applications that spreads requests across multiple servers. It is written in C language and thus the reputation for being fast and efficient.\nIf you are curious, this HAProxy Page offers more details about its features.\nInstallation In this guide , we will be installing the latest stable version of 1.8 (as of May 09, 2018). the current HAProxy available under RHEL is 1.5.18 and 1.8.8 on Debian/Ubuntu. before taking the next step, make sure you have gcc, pcre-static and pcre-devel installed:\n# CentOS RHEL: sudo yum -y install make gcc perl pcre-devel zlib-devel # Debian/Ubuntu: sudo apt install make gcc perl pcre-devel zlib-devel  Enable IP forwarding and binding to non-local IP addresses:\n# echo \u0026quot;net.ipv4.ip_forward = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf # echo \u0026quot;net.ipv4.ip_nonlocal_bind = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf # sysctl -p net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1  Download the source code:\nwget https://www.haproxy.org/download/1.8/src/haproxy-1.8.8.tar.gz  Extract the file and change directory:\ntar xvzf haproxy-1.8.8.tar.gz \u0026amp;\u0026amp; cd haproxy-1.8.8  Compile the program:\nmake TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1  This is the list of TARGETs:\n linux22 for Linux 2.2 linux24 for Linux 2.4 and above (default) linux24e for Linux 2.4 with support for a working epoll (\u0026gt; 0.21) linux26 for Linux 2.6 and above linux2628 for Linux 2.6.28, 3.x, and above (enables splice and tproxy) solaris for Solaris 8 or 10 (others untested) freebsd for FreeBSD 5 to 10 (others untested) netbsd for NetBSD osx for Mac OS/X openbsd for OpenBSD 5.7 and above aix51 for AIX 5.1 aix52 for AIX 5.2 cygwin for Cygwin haiku for Haiku generic for any other OS or version. custom to manually adjust every setting  You can also compile the program with following options: USE_PCRE=1 to use libpcre, in whatever form is available on your system (shared or static). USE_ZLIP=1 to use zlib compression Library. USE_OPENSSL=1 to add native support for SSL using the GNU makefile.\nInstall HAProxy:\nsudo make install  With that, we should now have the chosen HAProxy version installed. now we need to setup HAProxy.\nSetup: Adding HAProxy user:\nid -u haproxy \u0026amp;\u0026gt; /dev/null || useradd -s /usr/sbin/nologin -r haproxy  Copy HAProxy binary located in the extracted HAProxy directory to /usr/sbin/:\ncp haproxy /usr/sbin/  Create Manual page:\nwget -qO - https://raw.githubusercontent.com/horms/haproxy/master/doc/configuration.txt | gzip -c \u0026gt; /usr/share/doc/haproxy/configuration.txt.gz  Create Systemd service script to manage HAProxy, use the following at your discretion:\nsudo wget https://gist.githubusercontent.com/tmidi/1699a358533ae876513e2887fec6fbe2/raw/6c07ce39adc56c731d2bbeb88b90d8bbc636f3ea/haproxy.service -O /lib/systemd/system/haproxy.service  or create /lib/systemd/system/haproxy.service with the following content:\n[Unit] Description=HAProxy Load Balancer Documentation=man:haproxy(1) Documentation=file:/usr/share/doc/haproxy/configuration.txt.gz # allows us to do millisecond level restarts without triggering alert in Systemd StartLimitInterval=0 StartLimitBurst=0 After=network.target syslog.service Wants=syslog.service [Service] Environment=\u0026quot;CONFIG=/etc/haproxy/haproxy.cfg\u0026quot; \u0026quot;PIDFILE=/run/haproxy.pid\u0026quot; # EXTRAOPTS and RELOADOPS come from this default file EnvironmentFile=-/etc/default/haproxy ExecStartPre=/usr/sbin/haproxy -f $CONFIG -c -q ExecStart=/usr/sbin/haproxy -W -f $CONFIG -p $PIDFILE $EXTRAOPTS ExecReload=/usr/sbin/haproxy -f $CONFIG -c -q $EXTRAOPTS $RELOADOPTS ExecReload=/bin/kill -USR2 $MAINPID KillMode=mixed Restart=always Type=forking [Install] WantedBy=multi-user.target  Enable and start haproxy.service:\nsystemctl enablle haproxy.service systemctl start haproxy.service  Conclusion This conclude conclude HAProxy installation steps.This guide however covers only the installation steps, in future post I will demonstrate how to configure HAProxy to load balance three backend web servers while using sticky session and SSL Pass-through.\nIf you find this guide helpful, please share it with your friends and colleagues. I am on Twitter and LinkedIn, let's connect or let me know if you have any question or any suggestions.\n","id":1,"section":"posts","summary":"intro In this post, I will demonstrate how we can install HAProxy from source code. According to Wikipedia, HAProxy was written and still maintained by Willy Tarreau since 2000. HAProxy is free open source software (FOSS), that provides a high availability load balancer and proxy server for TCP (Transmission Control Protocol) and HTTP (Hypertext Transfer Protocol) based applications that spreads requests across multiple servers. It is written in C language and thus the reputation for being fast and efficient.","tags":null,"title":"Getting started with HAProxy: Install from source code","uri":"https://midiroot.com/getting-started-with-haproxy-install-from-source-code/","year":"2018"},{"content":"Donovan Brown wrote a good article showcasing how you can use a custom PowerShell function to navigate up directories without the need to type multiple cd .. . I find his idea interesting and could be of important time saver if you spend a lot of time working with PowerShell.\nUnfortunately (or fortunately depends how you see it) nowadays I don't use PowerShell a lot, and when I do my usage is limited to few PowerCLI commands, but I do spend a considerable time interacting with Linux Shell, so the natural thing to do is to port Donovan's idea into a Bash script. The Bash script is going to be different from the PowerShell one, but the concept remains the same.\nWhat we need?\n A function that take an integer as an argument. A help function to display a basic help menu when wrong argument entered.  The help function: I like to keep this as simple as possible, since our function will take only one line for the argument and the second line to indicate that this a help menu:\nI used -EOF to allow indentation, this work better with tabs than with spaces.\nThe back up function Now that our help function is out of the way we can start building the backup function. we know the function will take one argument, and this argument must be an integer:\nHow this works? the functions starts first by making sure the argument is a valid integer, if not it will call the help function we created earlier. Then using a sequence of numbers from 1 to the argument, we will append ../ string to STRARGMNT with each iteration. When the for loop is complete we run cd command with all the final appended up directories. This will give us:\n   argument command     1 cd ..   2 cd ../..   3 cd ../../..    How to use this? I usually add functions like these to a .functions file under my home directory, the function file get sources by .bash_profile. To source .functions or other dotfiles add this for loop to your .bash_profile, files must be comma separated:\nCreate .functions and add this content to it:\nThis is slightly different from the previous functions we created. The help menu is a nested function inside the main bu function. we also created empty STRARGMNT to unset the variable each time the function runs.\nWhen you are done source .bash_profile or close and reopen your terminal for the change to take effect\nhow to run? bu help or bu -h for help menu. bu 2 to go two folders up, this is the equivalent of running cd ..\\..\\ you can see bu in action here(external link):\n\n","id":2,"section":"posts","summary":"Donovan Brown wrote a good article showcasing how you can use a custom PowerShell function to navigate up directories without the need to type multiple cd .. . I find his idea interesting and could be of important time saver if you spend a lot of time working with PowerShell.\nUnfortunately (or fortunately depends how you see it) nowadays I don't use PowerShell a lot, and when I do my usage is limited to few PowerCLI commands, but I do spend a considerable time interacting with Linux Shell, so the natural thing to do is to port Donovan's idea into a Bash script.","tags":null,"title":"Change Directory Up Faster With Back up Command","uri":"https://midiroot.com/change-directory-up-faster-with-back-up-command/","year":"2018"},{"content":"According to Ansible's Best Practices, There are many possible ways to organize playbook content, and that the usage of such layout should fit your needs. The only thing I highly recommend is using roles instead of tasks, this will give your flexibility and better organization of your code.\nAnsible provide two examples of directory layouts. the first one is pretty simple and the one I go to when I am working on a small environement with a sample production and staging inventory files:\nproduction # inventory file for production servers staging # inventory file for staging environment group_vars/ group1 # here we assign variables to particular groups group2 # \u0026quot;\u0026quot; host_vars/ hostname1 # if systems need specific variables, put them here hostname2 # \u0026quot;\u0026quot; library/ # if any custom modules, put them here (optional) module_utils/ # if any custom module_utils to support modules, put them here (optional) filter_plugins/ # if any custom filter plugins, put them here (optional) site.yml # master playbook webservers.yml # playbook for webserver tier dbservers.yml # playbook for dbserver tier roles/ common/ # this hierarchy represents a \u0026quot;role\u0026quot; tasks/ # main.yml # \u0026lt;-- tasks file can include smaller files if warranted handlers/ # main.yml # \u0026lt;-- handlers file templates/ # \u0026lt;-- files for use with the template resource ntp.conf.j2 # \u0026lt;------- templates end in .j2 files/ # bar.txt # \u0026lt;-- files for use with the copy resource foo.sh # \u0026lt;-- script files for use with the script resource vars/ # main.yml # \u0026lt;-- variables associated with this role defaults/ # main.yml # \u0026lt;-- default lower priority variables for this role meta/ # main.yml # \u0026lt;-- role dependencies library/ # roles can also include custom modules module_utils/ # roles can also include custom module_utils lookup_plugins/ # or other types of plugins, like lookup in this case webtier/ # same kind of structure as \u0026quot;common\u0026quot; was above, done for the webtier role monitoring/ # \u0026quot;\u0026quot; fooapp/ # \u0026quot;\u0026quot;  I always use the following command to create the above directory structure and get started as soon as possible:\nWhen I have a more complex inventory, with multiple groups and children, I opt for this alternative directory layout:\ninventories/ production/ hosts # inventory file for production servers group_vars/ group1 # here we assign variables to particular groups group2 # \u0026quot;\u0026quot; host_vars/ hostname1 # if systems need specific variables, put them here hostname2 # \u0026quot;\u0026quot; staging/ hosts # inventory file for staging environment group_vars/ group1 # here we assign variables to particular groups group2 # \u0026quot;\u0026quot; host_vars/ stagehost1 # if systems need specific variables, put them here stagehost2 # \u0026quot;\u0026quot; library/ module_utils/ filter_plugins/ site.yml webservers.yml dbservers.yml roles/ common/ # this hierarchy represents a \u0026quot;role\u0026quot; tasks/ # main.yml # \u0026lt;-- tasks file can include smaller files if warranted handlers/ # main.yml # \u0026lt;-- handlers file templates/ # \u0026lt;-- files for use with the template resource ntp.conf.j2 # \u0026lt;------- templates end in .j2 files/ # bar.txt # \u0026lt;-- files for use with the copy resource foo.sh # \u0026lt;-- script files for use with the script resource vars/ # main.yml # \u0026lt;-- variables associated with this role defaults/ # main.yml # \u0026lt;-- default lower priority variables for this role meta/ # main.yml # \u0026lt;-- role dependencies library/ # roles can also include custom modules module_utils/ # roles can also include custom module_utils lookup_plugins/ # or other types of plugins, like lookup in this case  With this structure, I can put each inventory file with its group_vars\\host_vars in separate directory. To quickly spin up this directory layout, I use the following commands:\nI usually don't do any work on roles/common, to create a new role I just duplicate common to a new directory, this way I have a role template that I can use whenever I want to create a new role.\n","id":3,"section":"posts","summary":"According to Ansible's Best Practices, There are many possible ways to organize playbook content, and that the usage of such layout should fit your needs. The only thing I highly recommend is using roles instead of tasks, this will give your flexibility and better organization of your code.\nAnsible provide two examples of directory layouts. the first one is pretty simple and the one I go to when I am working on a small environement with a sample production and staging inventory files:","tags":null,"title":"Ansible: Directory Layout","uri":"https://midiroot.com/ansible-directory-layout/","year":"2018"},{"content":"In this post, we look dive into some of the interesting way we can use Bash to work with file names and paths, this usually helpful when you want to put a quick Bash script to copy or move/rename file. I recently had to deal with hundreds of log files generataed by the dozens of containers, the issue was each container writes logs inside a folder, but the logs files are basically have the same name inside each folder.\nAs you can see from the example above, if I want to backup the log files, and move them all to a remote server, I have two options:\n For each directory, I will create a mirror Directory and then move the log files there. Rename each file, in a way to show which directory it belongs to; ex Container_01-java.log or java-Container_01.log  For me, the second option works the best, this was I don't have to worry about multiple directory in the remote backup host, and have all the logs file under the same location. to achieve this, I had to do some path manipulation.\n I need to get the directory name, assuming our path is \u0026ldquo;/var/log/:  This will get the name of each directory inside the path we provided, and then save the value using variable BASE, which we can use at later point. the echo statement is not necessary, and it's there just for debugging. Note that the result of the loop is a full path, using ##*/ will give us the last\nNow for each directory we gathered from the previous run, we need to get the list of file inside each one, to achieve this:  Again with this we will get the full path, to manipulate this, we need to slice the full path, in one part we need the full path minus the filename (and the extension) and assign that to a variable, in the second part we need the short file name (nd the extension) and assign that a different variables. To get the Path - file name =\u0026gt; %/* To get the the file name =\u0026gt; ##*/ # This is similar to what we used to get BASE:\nRename/move files: All we have to do at this point is to combine the variables to move the files:  if we put all together, we will have the following snippet of code:\nSome other methods to manipulate files:\n","id":4,"section":"posts","summary":"In this post, we look dive into some of the interesting way we can use Bash to work with file names and paths, this usually helpful when you want to put a quick Bash script to copy or move/rename file. I recently had to deal with hundreds of log files generataed by the dozens of containers, the issue was each container writes logs inside a folder, but the logs files are basically have the same name inside each folder.","tags":null,"title":"Bash - Paths and file names manipulation","uri":"https://midiroot.com/bash-paths-and-file-names-manipulation/","year":"2018"},{"content":"What's Cryptsetup? According to Cryptsetup's Gitlab project page; Cryptsetup is utility used to conveniently setup disk encryption based on DMCrypt kernel module.\nThese include plain dm-crypt volumes, LUKS volumes, loop-AES and TrueCrypt (including VeraCrypt extension) format.\nProject also includes veritysetup utility used to conveniently setup DMVerity block integrity checking kernel module.\nTo install LUKS:\nActivate Dmcrypt:\nCreate the file to encrypt:\nTechnically we are converting and coping a file. if: input file, we are using /dev/zero to fill the file with null characters (ASCII NUL, 0x00). of: output file, Write to FILE instead of standard output. bs: Block size, for both read and write, default is 512. count: copy only N input blocks, in our example we will copy 1Mx1024, the output file size will be 1GB.\nFormat the new created file:\nThis will initializes a LUKS partition and sets the initial key. you need to remember the initialization key, this is the key you will use to mount or open the file\nNow, we need to open the LUKS partition:\nThis command will opens LUKS partition device and sets up a mapping name after successful verification of the initialization key.\nLet's create XFS file system, you can use other file systems, adjust the command accordingly:\nClose the LUKS partition:\nAt this point, you have an encrypted LUKS partition, now we need a mounting point to be able to access this partition, for this we need to open again the LUKS partition:\nWhen prompted, enter your password.\nCreate a mount point, I chose \u0026ldquo;/mnt/encrypted\u0026rdquo;:\nMount LUKS partition:\nif you issue \u0026ldquo;df -h\u0026rdquo; or \u0026ldquo;mount | grep safe-encrypt\u0026rdquo; you should be able to see newly mounted partition:\nWhen you are done working on the partition, unmount the file system then close the LUKS partition:\n  ","id":5,"section":"posts","summary":"What's Cryptsetup? According to Cryptsetup's Gitlab project page; Cryptsetup is utility used to conveniently setup disk encryption based on DMCrypt kernel module.\nThese include plain dm-crypt volumes, LUKS volumes, loop-AES and TrueCrypt (including VeraCrypt extension) format.\nProject also includes veritysetup utility used to conveniently setup DMVerity block integrity checking kernel module.\nTo install LUKS:\nActivate Dmcrypt:\nCreate the file to encrypt:\nTechnically we are converting and coping a file. if: input file, we are using /dev/zero to fill the file with null characters (ASCII NUL, 0x00).","tags":null,"title":"Cryptsetup Create Linux Encrypted Volumes","uri":"https://midiroot.com/cryptsetup-create-linux-encrypted-volumes/","year":"2017"},{"content":"Journald address one major issue with Linux applications logging, it provides a centralized management logging for the Kernel and the userland processes regardless where the logs are coming from. you can also use journald as an alternative looging driver inside Docker containers, this feature is available since Docker version 1.7.\nTo view logs written by journald you may use journalctl, it will show the full content of the journal when it's called without parameters:\nTo show all fields in full when the lines are very longs:\nUse -r or \u0026ndash;reverse to show newest entries first:\nYou can also use -n N or \u0026ndash;lines=N to show the last N numbers, -e or \u0026ndash;pager-end to view the end of the logs and -f or \u0026ndash;follow to stream continuously journal entries. One of may favorite options to run journalctl is -u or \u0026ndash;unit, this will show messages for the specific systemd unit entered or a matching pattern\nYou can use -p, \u0026ndash;priority to filter messages by priority, Takes either a single numeric or textual log level (i.e. between 0/\u0026ldquo;emerg\u0026rdquo; and 7/\u0026ldquo;debug\u0026rdquo;). Accepted priorities are : \u0026ldquo;emerg\u0026rdquo; (0), \u0026ldquo;alert\u0026rdquo; (1), \u0026ldquo;crit\u0026rdquo; (2), \u0026ldquo;err\u0026rdquo; (3), \u0026ldquo;warning\u0026rdquo; (4), \u0026ldquo;notice\u0026rdquo; (5), \u0026ldquo;info\u0026rdquo; (6), \u0026ldquo;debug\u0026rdquo; (7).\nWhen it is supported, you can use -x, \u0026ndash;catalog to show explanation texts from the message catalog, this will supply you with explanation of the message and a possible solution or point you to external resources to resolve an issue:\njournalctl in my opinion is a significant tool to use for debugging system and application issue, take the time to read the man page and practice using the above examples or from the man page.\nReferences:  journalctl man page.  ","id":6,"section":"posts","summary":"Journald address one major issue with Linux applications logging, it provides a centralized management logging for the Kernel and the userland processes regardless where the logs are coming from. you can also use journald as an alternative looging driver inside Docker containers, this feature is available since Docker version 1.7.\nTo view logs written by journald you may use journalctl, it will show the full content of the journal when it's called without parameters:","tags":null,"title":"Journalctl examples","uri":"https://midiroot.com/journalctl-examples/","year":"2017"},{"content":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.\nNow it's the time to empty the content of file, this helpful if your find rules did not catch some files, or you don't want to delete active log file.\nExplanation: This will empty the content of all files in /var/log/, concatenate /dev/null (/dev/null is a special file system object that hides the output) to each item.\nReferences:  find man page. This answer in Server Fault  ","id":7,"section":"posts","summary":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.","tags":null,"title":"Clear content of multiple files at once","uri":"https://midiroot.com/clear-content-of-multiple-files-at-once/","year":"2017"},{"content":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.\nNow it's the time to empty the content of file, this helpful if your find rules did not catch some files, or you don't want to delete active log file.\nExplanation: This will empty the content of all files in /var/log/, concatenate /dev/null (/dev/null is a special file system object that hides the output) to each item.\nReferences:  find man page. This answer in Server Fault  ","id":8,"section":"posts","summary":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.","tags":null,"title":"How to clear content of multiple files at once","uri":"https://midiroot.com/how-to-clear-content-of-multiple-files-at-once/","year":"2017"},{"content":"When SELinux enabled, some OSSEC packages will fail to rotate logs under /var/ossec/logs, which will result in crontab errors and in some cases failure to write to logs.\nOne way to fix this is to change context type, for this let's first check the current context:\nCommand explanation:  semanage fcontext : Used to change SELinux context of files. semanage fcontext -a: Add object to record name. semanage fcontext -t: SELinux type of Object. restorecon : -R for recursively and -v for verbose, or to show changes in file labels.  If above failed, don't disable SELinux, instead generate and install a SELinux targeted policy, audit2allow is your best friend in this case. Red Hat offers a good step by step or Dan's Walsh revisited guide to achieve this, if you are following Red Hat's guide, pleas keep in mind that you might have multiple denials, so you might need to Grep for the \u0026ldquo;comm\u0026rdquo; value to create a specific policy.\nReferences:  Semanage man page. Restorecon man page Audit2allow Red Hat guide. Dan Walsh's Blog Using audit2allow to build policy modules. Revisited.  ","id":9,"section":"posts","summary":"When SELinux enabled, some OSSEC packages will fail to rotate logs under /var/ossec/logs, which will result in crontab errors and in some cases failure to write to logs.\nOne way to fix this is to change context type, for this let's first check the current context:\nCommand explanation:  semanage fcontext : Used to change SELinux context of files. semanage fcontext -a: Add object to record name. semanage fcontext -t: SELinux type of Object.","tags":null,"title":"OSSEC - log rotation","uri":"https://midiroot.com/ossec-log-rotation/","year":"2017"},{"content":"Introduction Couple weeks ago, I took a journey to centralize all our infrastructure logs in one location for the seek of ease to teams who needs access such logs.\nDiversity is a good thing, but it's a challenge when you have multiple devices in your infrastructure that generate logs in different formats and standards. While looking at multiple solutions and comparing them.\nWe needed something that support Linux, Windows and dozens of network devices, a solution that easy to configure, automate and cost effective. With thousands of endpoints it must be of high performance and possible to scale when it is needed to. The quest lead us to the old rsyslog, it was the only solution that checked all the boxes with capability to deliver one million messages per second to local destinations and it is completely free.\nIn this post, I will cover some of the basic configurations to get you up an running with rsyslog and to make your configuration easier to automate and to change your rsyslog server as you add more endpoints.\nWorking with Rsyslog Installation rsyslog comes pre-installed in all major Linux distributions, if you need steps to install rsyslog, check Newbie guide to rsyslog to install from source, otherwise use your distro package manager to install it.\nDebian: sudo apt install rsyslog  RHEL: sudo yum install rsyslog  Configurations Rsyslogd's main configuration is typically found in /etc/rsyslog.conf, more configuration files can be placed in /etc/rsyslog.d/ for a modular configuration.\nFor this post, we will be creating modular configuration files for inputs, templates and rulesets.\nInputs In a rsyslog server, input is used to describe the message sources, input takes multiple parameters, we will be using few of them to define the port and type.\nI new config file called 01-inputs.conf create your first input:\ninput(type=\u0026quot;imudp\u0026quot; port=\u0026quot;514\u0026quot; ruleset=\u0026quot;remote_all\u0026quot;) input(type=\u0026quot;imtcp\u0026quot; port=\u0026quot;514\u0026quot; ruleset=\u0026quot;remote_all\u0026quot;)  With this; we created two inputs using default rsyslog port (514), one to accept TCP connections with type imtcp and the secoand one for UDP using imudp. For TCP, you can use imptcp which tailored for high performance and it is available since versions 4.7.3+, 5.5.8+.\nruleset is used to define the ruleset that will be used with this input, we will define one in our output config file.\nYou can add other inputs as you need as long as the port you picked is not used by another process.\nTemplates Templates allow you to specify the format of the message or used for dynamic file name generation which we will ne using in our 01-templates.conf file:\ntemplate(name=\u0026quot;catch_perhost\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/%FROMHOST%.log\u0026quot;)  We are using here a template of type string. list, subtree and plugin are also available as template types. We are generating dynamic file names using %FROMHOST% as a string the file name. you can change that and use static name if you want to send logs from a specific group of devices:\ntemplate(name=\u0026quot;catch_asa\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/asa.log\u0026quot;) template(name=\u0026quot;catch_vcenter\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/vcenter.log\u0026quot;) template(name=\u0026quot;catch_all\u0026quot; type=\u0026quot;string\u0026quot; string=\u0026quot;/var/log/remote/catch_all.log\u0026quot;)  Notice the template name change which will give us later more flexibility when it comes to our rulesets. When choosing template name please avoiding using RSYSLOG_ as they reserved for rsyslog use and will cause conflict.\nI am using catch_all template to process all unfiltered messages, this help in logging all messages that escape filters so I can review them and see what needs to be adjusted to filter them.\nRulesets Rulesets consists of a filter and action(s) to be executed when the filter evaluate to true. A filter evaluated using could a priority and/or facility, other property-based filters could be used like msg, syslogtag, hostname or host IP.\nIn our 03-rulesets.conf, we will define a ruleset for processing messages coming from an IP address that belongs to a vCenter host and places messages in vcenter.log file that we defined in our templates files.\nruleset(name=\u0026quot;remote_all\u0026quot; queue.type=\u0026quot;LinkedList\u0026quot; queue.size=\u0026quot;100000\u0026quot;) { if ($fromhost-ip == '192.168.33.11') then { action(type=\u0026quot;omfile\u0026quot; DynaFile=\u0026quot;catch_vcenter\u0026quot; FileCreateMode=\u0026quot;0664\u0026quot;) stop } # This setup the catch all logging action(type=\u0026quot;omfile\u0026quot; DynaFile=\u0026quot;catch_all\u0026quot;) }  A lot to unpack here, we are using remote_all as the ruleset name, that's the same values we used when defining what ruleset the input will be using. LinkedList is in-memory queue type. when using LinkedList queue; the memory is allocated only when needed which make this type of queue handle message bursts very well. In our filter, we check if the $fromhost-ip is equal to IP address of our vCenter, if true; we use omfile type to write messages to files using catch_vcenter as a dynamic file template. FileCreateMode sets permissions for the dynamic file we just created.\nConclusion Rsyslog is a powerful logging solution and we barely scratched the surface of all the hundreds of available options/parameters we could use for configuration. It's always a good practice to test configuration in your testing environment and see what rsyslog documentation has to offer. If you see that I could have something differently in my config, please reach out and let me know, I am always looking for improvement.\n","id":10,"section":"posts","summary":"Introduction Couple weeks ago, I took a journey to centralize all our infrastructure logs in one location for the seek of ease to teams who needs access such logs.\nDiversity is a good thing, but it's a challenge when you have multiple devices in your infrastructure that generate logs in different formats and standards. While looking at multiple solutions and comparing them.\nWe needed something that support Linux, Windows and dozens of network devices, a solution that easy to configure, automate and cost effective.","tags":["rsyslog Linux Guide"],"title":"Working With Rsyslog","uri":"https://midiroot.com/working-with-rsyslog/","year":"2019"},{"content":"intro In this post, I will demonstrate how we can install HAProxy from source code. According to Wikipedia, HAProxy was written and still maintained by Willy Tarreau since 2000. HAProxy is free open source software (FOSS), that provides a high availability load balancer and proxy server for TCP (Transmission Control Protocol) and HTTP (Hypertext Transfer Protocol) based applications that spreads requests across multiple servers. It is written in C language and thus the reputation for being fast and efficient.\nIf you are curious, this HAProxy Page offers more details about its features.\nInstallation In this guide , we will be installing the latest stable version of 1.8 (as of May 09, 2018). the current HAProxy available under RHEL is 1.5.18 and 1.8.8 on Debian/Ubuntu. before taking the next step, make sure you have gcc, pcre-static and pcre-devel installed:\n# CentOS RHEL: sudo yum -y install make gcc perl pcre-devel zlib-devel # Debian/Ubuntu: sudo apt install make gcc perl pcre-devel zlib-devel  Enable IP forwarding and binding to non-local IP addresses:\n# echo \u0026quot;net.ipv4.ip_forward = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf # echo \u0026quot;net.ipv4.ip_nonlocal_bind = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf # sysctl -p net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1  Download the source code:\nwget https://www.haproxy.org/download/1.8/src/haproxy-1.8.8.tar.gz  Extract the file and change directory:\ntar xvzf haproxy-1.8.8.tar.gz \u0026amp;\u0026amp; cd haproxy-1.8.8  Compile the program:\nmake TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1  This is the list of TARGETs:\n linux22 for Linux 2.2 linux24 for Linux 2.4 and above (default) linux24e for Linux 2.4 with support for a working epoll (\u0026gt; 0.21) linux26 for Linux 2.6 and above linux2628 for Linux 2.6.28, 3.x, and above (enables splice and tproxy) solaris for Solaris 8 or 10 (others untested) freebsd for FreeBSD 5 to 10 (others untested) netbsd for NetBSD osx for Mac OS/X openbsd for OpenBSD 5.7 and above aix51 for AIX 5.1 aix52 for AIX 5.2 cygwin for Cygwin haiku for Haiku generic for any other OS or version. custom to manually adjust every setting  You can also compile the program with following options: USE_PCRE=1 to use libpcre, in whatever form is available on your system (shared or static). USE_ZLIP=1 to use zlib compression Library. USE_OPENSSL=1 to add native support for SSL using the GNU makefile.\nInstall HAProxy:\nsudo make install  With that, we should now have the chosen HAProxy version installed. now we need to setup HAProxy.\nSetup: Adding HAProxy user:\nid -u haproxy \u0026amp;\u0026gt; /dev/null || useradd -s /usr/sbin/nologin -r haproxy  Copy HAProxy binary located in the extracted HAProxy directory to /usr/sbin/:\ncp haproxy /usr/sbin/  Create Manual page:\nwget -qO - https://raw.githubusercontent.com/horms/haproxy/master/doc/configuration.txt | gzip -c \u0026gt; /usr/share/doc/haproxy/configuration.txt.gz  Create Systemd service script to manage HAProxy, use the following at your discretion:\nsudo wget https://gist.githubusercontent.com/tmidi/1699a358533ae876513e2887fec6fbe2/raw/6c07ce39adc56c731d2bbeb88b90d8bbc636f3ea/haproxy.service -O /lib/systemd/system/haproxy.service  or create /lib/systemd/system/haproxy.service with the following content:\n[Unit] Description=HAProxy Load Balancer Documentation=man:haproxy(1) Documentation=file:/usr/share/doc/haproxy/configuration.txt.gz # allows us to do millisecond level restarts without triggering alert in Systemd StartLimitInterval=0 StartLimitBurst=0 After=network.target syslog.service Wants=syslog.service [Service] Environment=\u0026quot;CONFIG=/etc/haproxy/haproxy.cfg\u0026quot; \u0026quot;PIDFILE=/run/haproxy.pid\u0026quot; # EXTRAOPTS and RELOADOPS come from this default file EnvironmentFile=-/etc/default/haproxy ExecStartPre=/usr/sbin/haproxy -f $CONFIG -c -q ExecStart=/usr/sbin/haproxy -W -f $CONFIG -p $PIDFILE $EXTRAOPTS ExecReload=/usr/sbin/haproxy -f $CONFIG -c -q $EXTRAOPTS $RELOADOPTS ExecReload=/bin/kill -USR2 $MAINPID KillMode=mixed Restart=always Type=forking [Install] WantedBy=multi-user.target  Enable and start haproxy.service:\nsystemctl enablle haproxy.service systemctl start haproxy.service  Conclusion This conclude conclude HAProxy installation steps.This guide however covers only the installation steps, in future post I will demonstrate how to configure HAProxy to load balance three backend web servers while using sticky session and SSL Pass-through.\nIf you find this guide helpful, please share it with your friends and colleagues. I am on Twitter and LinkedIn, let's connect or let me know if you have any question or any suggestions.\n","id":11,"section":"posts","summary":"intro In this post, I will demonstrate how we can install HAProxy from source code. According to Wikipedia, HAProxy was written and still maintained by Willy Tarreau since 2000. HAProxy is free open source software (FOSS), that provides a high availability load balancer and proxy server for TCP (Transmission Control Protocol) and HTTP (Hypertext Transfer Protocol) based applications that spreads requests across multiple servers. It is written in C language and thus the reputation for being fast and efficient.","tags":null,"title":"Getting started with HAProxy: Install from source code","uri":"https://midiroot.com/getting-started-with-haproxy-install-from-source-code/","year":"2018"},{"content":"Donovan Brown wrote a good article showcasing how you can use a custom PowerShell function to navigate up directories without the need to type multiple cd .. . I find his idea interesting and could be of important time saver if you spend a lot of time working with PowerShell.\nUnfortunately (or fortunately depends how you see it) nowadays I don't use PowerShell a lot, and when I do my usage is limited to few PowerCLI commands, but I do spend a considerable time interacting with Linux Shell, so the natural thing to do is to port Donovan's idea into a Bash script. The Bash script is going to be different from the PowerShell one, but the concept remains the same.\nWhat we need?\n A function that take an integer as an argument. A help function to display a basic help menu when wrong argument entered.  The help function: I like to keep this as simple as possible, since our function will take only one line for the argument and the second line to indicate that this a help menu:\nI used -EOF to allow indentation, this work better with tabs than with spaces.\nThe back up function Now that our help function is out of the way we can start building the backup function. we know the function will take one argument, and this argument must be an integer:\nHow this works? the functions starts first by making sure the argument is a valid integer, if not it will call the help function we created earlier. Then using a sequence of numbers from 1 to the argument, we will append ../ string to STRARGMNT with each iteration. When the for loop is complete we run cd command with all the final appended up directories. This will give us:\n   argument command     1 cd ..   2 cd ../..   3 cd ../../..    How to use this? I usually add functions like these to a .functions file under my home directory, the function file get sources by .bash_profile. To source .functions or other dotfiles add this for loop to your .bash_profile, files must be comma separated:\nCreate .functions and add this content to it:\nThis is slightly different from the previous functions we created. The help menu is a nested function inside the main bu function. we also created empty STRARGMNT to unset the variable each time the function runs.\nWhen you are done source .bash_profile or close and reopen your terminal for the change to take effect\nhow to run? bu help or bu -h for help menu. bu 2 to go two folders up, this is the equivalent of running cd ..\\..\\ you can see bu in action here(external link):\n\n","id":12,"section":"posts","summary":"Donovan Brown wrote a good article showcasing how you can use a custom PowerShell function to navigate up directories without the need to type multiple cd .. . I find his idea interesting and could be of important time saver if you spend a lot of time working with PowerShell.\nUnfortunately (or fortunately depends how you see it) nowadays I don't use PowerShell a lot, and when I do my usage is limited to few PowerCLI commands, but I do spend a considerable time interacting with Linux Shell, so the natural thing to do is to port Donovan's idea into a Bash script.","tags":null,"title":"Change Directory Up Faster With Back up Command","uri":"https://midiroot.com/change-directory-up-faster-with-back-up-command/","year":"2018"},{"content":"According to Ansible's Best Practices, There are many possible ways to organize playbook content, and that the usage of such layout should fit your needs. The only thing I highly recommend is using roles instead of tasks, this will give your flexibility and better organization of your code.\nAnsible provide two examples of directory layouts. the first one is pretty simple and the one I go to when I am working on a small environement with a sample production and staging inventory files:\nproduction # inventory file for production servers staging # inventory file for staging environment group_vars/ group1 # here we assign variables to particular groups group2 # \u0026quot;\u0026quot; host_vars/ hostname1 # if systems need specific variables, put them here hostname2 # \u0026quot;\u0026quot; library/ # if any custom modules, put them here (optional) module_utils/ # if any custom module_utils to support modules, put them here (optional) filter_plugins/ # if any custom filter plugins, put them here (optional) site.yml # master playbook webservers.yml # playbook for webserver tier dbservers.yml # playbook for dbserver tier roles/ common/ # this hierarchy represents a \u0026quot;role\u0026quot; tasks/ # main.yml # \u0026lt;-- tasks file can include smaller files if warranted handlers/ # main.yml # \u0026lt;-- handlers file templates/ # \u0026lt;-- files for use with the template resource ntp.conf.j2 # \u0026lt;------- templates end in .j2 files/ # bar.txt # \u0026lt;-- files for use with the copy resource foo.sh # \u0026lt;-- script files for use with the script resource vars/ # main.yml # \u0026lt;-- variables associated with this role defaults/ # main.yml # \u0026lt;-- default lower priority variables for this role meta/ # main.yml # \u0026lt;-- role dependencies library/ # roles can also include custom modules module_utils/ # roles can also include custom module_utils lookup_plugins/ # or other types of plugins, like lookup in this case webtier/ # same kind of structure as \u0026quot;common\u0026quot; was above, done for the webtier role monitoring/ # \u0026quot;\u0026quot; fooapp/ # \u0026quot;\u0026quot;  I always use the following command to create the above directory structure and get started as soon as possible:\nWhen I have a more complex inventory, with multiple groups and children, I opt for this alternative directory layout:\ninventories/ production/ hosts # inventory file for production servers group_vars/ group1 # here we assign variables to particular groups group2 # \u0026quot;\u0026quot; host_vars/ hostname1 # if systems need specific variables, put them here hostname2 # \u0026quot;\u0026quot; staging/ hosts # inventory file for staging environment group_vars/ group1 # here we assign variables to particular groups group2 # \u0026quot;\u0026quot; host_vars/ stagehost1 # if systems need specific variables, put them here stagehost2 # \u0026quot;\u0026quot; library/ module_utils/ filter_plugins/ site.yml webservers.yml dbservers.yml roles/ common/ # this hierarchy represents a \u0026quot;role\u0026quot; tasks/ # main.yml # \u0026lt;-- tasks file can include smaller files if warranted handlers/ # main.yml # \u0026lt;-- handlers file templates/ # \u0026lt;-- files for use with the template resource ntp.conf.j2 # \u0026lt;------- templates end in .j2 files/ # bar.txt # \u0026lt;-- files for use with the copy resource foo.sh # \u0026lt;-- script files for use with the script resource vars/ # main.yml # \u0026lt;-- variables associated with this role defaults/ # main.yml # \u0026lt;-- default lower priority variables for this role meta/ # main.yml # \u0026lt;-- role dependencies library/ # roles can also include custom modules module_utils/ # roles can also include custom module_utils lookup_plugins/ # or other types of plugins, like lookup in this case  With this structure, I can put each inventory file with its group_vars\\host_vars in separate directory. To quickly spin up this directory layout, I use the following commands:\nI usually don't do any work on roles/common, to create a new role I just duplicate common to a new directory, this way I have a role template that I can use whenever I want to create a new role.\n","id":13,"section":"posts","summary":"According to Ansible's Best Practices, There are many possible ways to organize playbook content, and that the usage of such layout should fit your needs. The only thing I highly recommend is using roles instead of tasks, this will give your flexibility and better organization of your code.\nAnsible provide two examples of directory layouts. the first one is pretty simple and the one I go to when I am working on a small environement with a sample production and staging inventory files:","tags":null,"title":"Ansible: Directory Layout","uri":"https://midiroot.com/ansible-directory-layout/","year":"2018"},{"content":"In this post, we look dive into some of the interesting way we can use Bash to work with file names and paths, this usually helpful when you want to put a quick Bash script to copy or move/rename file. I recently had to deal with hundreds of log files generataed by the dozens of containers, the issue was each container writes logs inside a folder, but the logs files are basically have the same name inside each folder.\nAs you can see from the example above, if I want to backup the log files, and move them all to a remote server, I have two options:\n For each directory, I will create a mirror Directory and then move the log files there. Rename each file, in a way to show which directory it belongs to; ex Container_01-java.log or java-Container_01.log  For me, the second option works the best, this was I don't have to worry about multiple directory in the remote backup host, and have all the logs file under the same location. to achieve this, I had to do some path manipulation.\n I need to get the directory name, assuming our path is \u0026ldquo;/var/log/:  This will get the name of each directory inside the path we provided, and then save the value using variable BASE, which we can use at later point. the echo statement is not necessary, and it's there just for debugging. Note that the result of the loop is a full path, using ##*/ will give us the last\nNow for each directory we gathered from the previous run, we need to get the list of file inside each one, to achieve this:  Again with this we will get the full path, to manipulate this, we need to slice the full path, in one part we need the full path minus the filename (and the extension) and assign that to a variable, in the second part we need the short file name (nd the extension) and assign that a different variables. To get the Path - file name =\u0026gt; %/* To get the the file name =\u0026gt; ##*/ # This is similar to what we used to get BASE:\nRename/move files: All we have to do at this point is to combine the variables to move the files:  if we put all together, we will have the following snippet of code:\nSome other methods to manipulate files:\n","id":14,"section":"posts","summary":"In this post, we look dive into some of the interesting way we can use Bash to work with file names and paths, this usually helpful when you want to put a quick Bash script to copy or move/rename file. I recently had to deal with hundreds of log files generataed by the dozens of containers, the issue was each container writes logs inside a folder, but the logs files are basically have the same name inside each folder.","tags":null,"title":"Bash - Paths and file names manipulation","uri":"https://midiroot.com/bash-paths-and-file-names-manipulation/","year":"2018"},{"content":"What's Cryptsetup? According to Cryptsetup's Gitlab project page; Cryptsetup is utility used to conveniently setup disk encryption based on DMCrypt kernel module.\nThese include plain dm-crypt volumes, LUKS volumes, loop-AES and TrueCrypt (including VeraCrypt extension) format.\nProject also includes veritysetup utility used to conveniently setup DMVerity block integrity checking kernel module.\nTo install LUKS:\nActivate Dmcrypt:\nCreate the file to encrypt:\nTechnically we are converting and coping a file. if: input file, we are using /dev/zero to fill the file with null characters (ASCII NUL, 0x00). of: output file, Write to FILE instead of standard output. bs: Block size, for both read and write, default is 512. count: copy only N input blocks, in our example we will copy 1Mx1024, the output file size will be 1GB.\nFormat the new created file:\nThis will initializes a LUKS partition and sets the initial key. you need to remember the initialization key, this is the key you will use to mount or open the file\nNow, we need to open the LUKS partition:\nThis command will opens LUKS partition device and sets up a mapping name after successful verification of the initialization key.\nLet's create XFS file system, you can use other file systems, adjust the command accordingly:\nClose the LUKS partition:\nAt this point, you have an encrypted LUKS partition, now we need a mounting point to be able to access this partition, for this we need to open again the LUKS partition:\nWhen prompted, enter your password.\nCreate a mount point, I chose \u0026ldquo;/mnt/encrypted\u0026rdquo;:\nMount LUKS partition:\nif you issue \u0026ldquo;df -h\u0026rdquo; or \u0026ldquo;mount | grep safe-encrypt\u0026rdquo; you should be able to see newly mounted partition:\nWhen you are done working on the partition, unmount the file system then close the LUKS partition:\n  ","id":15,"section":"posts","summary":"What's Cryptsetup? According to Cryptsetup's Gitlab project page; Cryptsetup is utility used to conveniently setup disk encryption based on DMCrypt kernel module.\nThese include plain dm-crypt volumes, LUKS volumes, loop-AES and TrueCrypt (including VeraCrypt extension) format.\nProject also includes veritysetup utility used to conveniently setup DMVerity block integrity checking kernel module.\nTo install LUKS:\nActivate Dmcrypt:\nCreate the file to encrypt:\nTechnically we are converting and coping a file. if: input file, we are using /dev/zero to fill the file with null characters (ASCII NUL, 0x00).","tags":null,"title":"Cryptsetup Create Linux Encrypted Volumes","uri":"https://midiroot.com/cryptsetup-create-linux-encrypted-volumes/","year":"2017"},{"content":"Journald address one major issue with Linux applications logging, it provides a centralized management logging for the Kernel and the userland processes regardless where the logs are coming from. you can also use journald as an alternative looging driver inside Docker containers, this feature is available since Docker version 1.7.\nTo view logs written by journald you may use journalctl, it will show the full content of the journal when it's called without parameters:\nTo show all fields in full when the lines are very longs:\nUse -r or \u0026ndash;reverse to show newest entries first:\nYou can also use -n N or \u0026ndash;lines=N to show the last N numbers, -e or \u0026ndash;pager-end to view the end of the logs and -f or \u0026ndash;follow to stream continuously journal entries. One of may favorite options to run journalctl is -u or \u0026ndash;unit, this will show messages for the specific systemd unit entered or a matching pattern\nYou can use -p, \u0026ndash;priority to filter messages by priority, Takes either a single numeric or textual log level (i.e. between 0/\u0026ldquo;emerg\u0026rdquo; and 7/\u0026ldquo;debug\u0026rdquo;). Accepted priorities are : \u0026ldquo;emerg\u0026rdquo; (0), \u0026ldquo;alert\u0026rdquo; (1), \u0026ldquo;crit\u0026rdquo; (2), \u0026ldquo;err\u0026rdquo; (3), \u0026ldquo;warning\u0026rdquo; (4), \u0026ldquo;notice\u0026rdquo; (5), \u0026ldquo;info\u0026rdquo; (6), \u0026ldquo;debug\u0026rdquo; (7).\nWhen it is supported, you can use -x, \u0026ndash;catalog to show explanation texts from the message catalog, this will supply you with explanation of the message and a possible solution or point you to external resources to resolve an issue:\njournalctl in my opinion is a significant tool to use for debugging system and application issue, take the time to read the man page and practice using the above examples or from the man page.\nReferences:  journalctl man page.  ","id":16,"section":"posts","summary":"Journald address one major issue with Linux applications logging, it provides a centralized management logging for the Kernel and the userland processes regardless where the logs are coming from. you can also use journald as an alternative looging driver inside Docker containers, this feature is available since Docker version 1.7.\nTo view logs written by journald you may use journalctl, it will show the full content of the journal when it's called without parameters:","tags":null,"title":"Journalctl examples","uri":"https://midiroot.com/journalctl-examples/","year":"2017"},{"content":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.\nNow it's the time to empty the content of file, this helpful if your find rules did not catch some files, or you don't want to delete active log file.\nExplanation: This will empty the content of all files in /var/log/, concatenate /dev/null (/dev/null is a special file system object that hides the output) to each item.\nReferences:  find man page. This answer in Server Fault  ","id":17,"section":"posts","summary":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.","tags":null,"title":"Clear content of multiple files at once","uri":"https://midiroot.com/clear-content-of-multiple-files-at-once/","year":"2017"},{"content":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.\nNow it's the time to empty the content of file, this helpful if your find rules did not catch some files, or you don't want to delete active log file.\nExplanation: This will empty the content of all files in /var/log/, concatenate /dev/null (/dev/null is a special file system object that hides the output) to each item.\nReferences:  find man page. This answer in Server Fault  ","id":18,"section":"posts","summary":"When I am updating my Linux template, I like to delete all old logs, as a first step, I delete old rotated logs. First check how your logs are rotated and create a find command the one below to delete old rotated logs:\nThe command above will delete all file that match the rule, you can limit how deep you want to find files using maxdepth. you can also use mtime +n to find files older than n days and delete them.","tags":null,"title":"How to clear content of multiple files at once","uri":"https://midiroot.com/how-to-clear-content-of-multiple-files-at-once/","year":"2017"},{"content":"When SELinux enabled, some OSSEC packages will fail to rotate logs under /var/ossec/logs, which will result in crontab errors and in some cases failure to write to logs.\nOne way to fix this is to change context type, for this let's first check the current context:\nCommand explanation:  semanage fcontext : Used to change SELinux context of files. semanage fcontext -a: Add object to record name. semanage fcontext -t: SELinux type of Object. restorecon : -R for recursively and -v for verbose, or to show changes in file labels.  If above failed, don't disable SELinux, instead generate and install a SELinux targeted policy, audit2allow is your best friend in this case. Red Hat offers a good step by step or Dan's Walsh revisited guide to achieve this, if you are following Red Hat's guide, pleas keep in mind that you might have multiple denials, so you might need to Grep for the \u0026ldquo;comm\u0026rdquo; value to create a specific policy.\nReferences:  Semanage man page. Restorecon man page Audit2allow Red Hat guide. Dan Walsh's Blog Using audit2allow to build policy modules. Revisited.  ","id":19,"section":"posts","summary":"When SELinux enabled, some OSSEC packages will fail to rotate logs under /var/ossec/logs, which will result in crontab errors and in some cases failure to write to logs.\nOne way to fix this is to change context type, for this let's first check the current context:\nCommand explanation:  semanage fcontext : Used to change SELinux context of files. semanage fcontext -a: Add object to record name. semanage fcontext -t: SELinux type of Object.","tags":null,"title":"OSSEC - log rotation","uri":"https://midiroot.com/ossec-log-rotation/","year":"2017"}],"tags":[{"title":"rsyslog Linux Guide","uri":"https://midiroot.com/tags/rsyslog-linux-guide/"}]}